{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2ae2f8-cda1-4a2f-87a8-a1b97340c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9fec66-1c6d-4b79-90c5-03f8d2d0e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Data Preprocessing\n",
    "# ====================================\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\", 2: \"<SOS>\", 3: \"<EOS>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word_count = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "        \n",
    "        self.word_count = dict(frequencies)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "    \n",
    "        # Step 1: Protect numbers with decimals and percentages\n",
    "        # Replace \"26.3%\" with \"26.3_percent\" (treat as a single token)\n",
    "        text = re.sub(r'(\\d+\\.\\d+)%', r'\\1_percent', text)  # 26.3% → 26.3_percent\n",
    "        text = re.sub(r'(\\d+)%', r'\\1_percent', text)       # 5% → 5_percent\n",
    "        text = re.sub(r'(\\d+)\\.(\\d+)', r'\\1.\\2', text)      # 1.5 → 1.5 (unchanged)\n",
    "    \n",
    "        # Step 2: Remove unwanted punctuation (except protected cases)\n",
    "        # Keep: letters, numbers, underscores, and protected tokens (e.g., 26.3_percent)\n",
    "        text = re.sub(r'[^\\w\\s.]', '', text)  # Allow dots (.) in numbers\n",
    "    \n",
    "        # Step 3: Split into tokens\n",
    "        tokens = text.split()\n",
    "    \n",
    "        return tokens\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        \n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def save_vocab(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'itos': self.itos,\n",
    "                'stoi': self.stoi,\n",
    "                'word_count': self.word_count\n",
    "            }, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vocab(cls, path):\n",
    "        vocab = cls()\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            vocab.itos = data['itos']\n",
    "            vocab.stoi = data['stoi']\n",
    "            vocab.word_count = data['word_count']\n",
    "        return vocab\n",
    "\n",
    "\n",
    "class FinancialCausalDataset(Dataset):\n",
    "    def __init__(self, texts, causes, effects, vocab, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.causes = causes\n",
    "        self.effects = effects\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        cause = self.causes[index]\n",
    "        effect = self.effects[index]\n",
    "        \n",
    "        # Numericalize text\n",
    "        numeric_text = self.vocab.numericalize(text)\n",
    "        \n",
    "        # Truncating if necessary\n",
    "        if len(numeric_text) > self.max_len:\n",
    "            numeric_text = numeric_text[:self.max_len]\n",
    "        \n",
    "        text_length = len(numeric_text)\n",
    "        \n",
    "        # Creating a cause span representation (start, end indices in the text)\n",
    "        cause_tokens = self.vocab.tokenize(cause)\n",
    "        text_tokens = self.vocab.tokenize(text)\n",
    "        \n",
    "        cause_start, cause_end = -1, -1\n",
    "        for i in range(len(text_tokens) - len(cause_tokens) + 1):\n",
    "            if text_tokens[i:i+len(cause_tokens)] == cause_tokens:\n",
    "                cause_start = i\n",
    "                cause_end = i + len(cause_tokens) - 1\n",
    "                break\n",
    "        \n",
    "        effect_tokens = self.vocab.tokenize(effect)\n",
    "        effect_start, effect_end = -1, -1\n",
    "        for i in range(len(text_tokens) - len(effect_tokens) + 1):\n",
    "            if text_tokens[i:i+len(effect_tokens)] == effect_tokens:\n",
    "                effect_start = i\n",
    "                effect_end = i + len(effect_tokens) - 1\n",
    "                break\n",
    "        \n",
    "        cause_mask = [0] * len(numeric_text)\n",
    "        effect_mask = [0] * len(numeric_text)\n",
    "        \n",
    "        if cause_start >= 0 and cause_end < len(numeric_text):\n",
    "            for i in range(cause_start, cause_end + 1):\n",
    "                if i < len(cause_mask):\n",
    "                    cause_mask[i] = 1\n",
    "                    \n",
    "        if effect_start >= 0 and effect_end < len(numeric_text):\n",
    "            for i in range(effect_start, effect_end + 1):\n",
    "                if i < len(effect_mask):\n",
    "                    effect_mask[i] = 1\n",
    "        \n",
    "        return {\n",
    "            \"text\": torch.tensor(numeric_text),\n",
    "            \"text_length\": text_length,\n",
    "            \"cause_mask\": torch.tensor(cause_mask),\n",
    "            \"effect_mask\": torch.tensor(effect_mask),\n",
    "            \"original_text\": text,\n",
    "            \"original_cause\": cause,\n",
    "            \"original_effect\": effect\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batch.sort(key=lambda x: x[\"text_length\"], reverse=True)\n",
    "    \n",
    "    text = [item[\"text\"] for item in batch]\n",
    "    text_lengths = [item[\"text_length\"] for item in batch]\n",
    "    cause_masks = [item[\"cause_mask\"] for item in batch]\n",
    "    effect_masks = [item[\"effect_mask\"] for item in batch]\n",
    "    original_texts = [item[\"original_text\"] for item in batch]\n",
    "    original_causes = [item[\"original_cause\"] for item in batch]\n",
    "    original_effects = [item[\"original_effect\"] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_text = pad_sequence(text, batch_first=True, padding_value=0)\n",
    "    padded_cause_masks = pad_sequence(cause_masks, batch_first=True, padding_value=0)\n",
    "    padded_effect_masks = pad_sequence(effect_masks, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        \"text\": padded_text,\n",
    "        \"text_lengths\": torch.tensor(text_lengths),\n",
    "        \"cause_masks\": padded_cause_masks,\n",
    "        \"effect_masks\": padded_effect_masks,\n",
    "        \"original_texts\": original_texts,\n",
    "        \"original_causes\": original_causes,\n",
    "        \"original_effects\": original_effects\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_data(csv_path, test_size=0.2, vocab_save_path=\"financial_vocab.pkl\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.head(20000)\n",
    "    required_columns = [\"Text\", \"Cause\", \"Effect\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found in the CSV file\")\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Building the vocabulary on training data only\n",
    "    vocab = Vocabulary(freq_threshold=2)\n",
    "    vocab.build_vocabulary(train_df[\"Text\"].tolist())\n",
    "    \n",
    "    # Saving vocabulary\n",
    "    vocab.save_vocab(vocab_save_path)\n",
    "    \n",
    "    # Creating train and test datasets\n",
    "    train_dataset = FinancialCausalDataset(\n",
    "        texts=train_df[\"Text\"].tolist(),\n",
    "        causes=train_df[\"Cause\"].tolist(),\n",
    "        effects=train_df[\"Effect\"].tolist(),\n",
    "        vocab=vocab\n",
    "    )\n",
    "    \n",
    "    test_dataset = FinancialCausalDataset(\n",
    "        texts=test_df[\"Text\"].tolist(),\n",
    "        causes=test_df[\"Cause\"].tolist(),\n",
    "        effects=test_df[\"Effect\"].tolist(),\n",
    "        vocab=vocab\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset, vocab\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Model Architecture\n",
    "# ====================================\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CausalAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(next(self.parameters()).device)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value: [batch_size, seq_len, hidden_dim]\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len = query.shape[1]\n",
    "        \n",
    "        Q = self.query(query)  # [batch_size, seq_len, hidden_dim]\n",
    "        K = self.key(key)      # [batch_size, seq_len, hidden_dim]\n",
    "        V = self.value(value)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Q @ K^T / sqrt(hidden_dim)\n",
    "        energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Softmax\n",
    "        attention = torch.softmax(energy, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        x = torch.matmul(attention, V)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        return x, attention\n",
    "\n",
    "\n",
    "class CausalContextLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super(CausalContextLayer, self).__init__()\n",
    "        assert hidden_dim % num_heads == 0\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.fc_q(query)  # [batch_size, seq_len, hidden_dim]\n",
    "        K = self.fc_k(key)    # [batch_size, seq_len, hidden_dim]\n",
    "        V = self.fc_v(value)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Self-attention\n",
    "        self.scale = self.scale.to(query.device)\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # Softmax\n",
    "        attention = torch.softmax(energy, dim=-1)    # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        x = torch.matmul(attention, V)               # [batch_size, num_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Reshape back\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()       # [batch_size, seq_len, num_heads, head_dim]\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Final linear layer\n",
    "        x = self.fc_o(x)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class FinancialCausalDetector(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=768, num_layers=3, \n",
    "                 num_heads=8, dropout=0.3, use_glove=False, glove_path=None):\n",
    "        super(FinancialCausalDetector, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM for encoding\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim // 2,  # Bidirectional, so each direction gets half\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Context encoding layers\n",
    "        self.context_layers = nn.ModuleList([\n",
    "            CausalContextLayer(hidden_dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.context_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Feed-forward layers after each context layer\n",
    "        self.ff_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim * 4, hidden_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Feed-forward norms\n",
    "        self.ff_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Cause-Effect span prediction\n",
    "        self.cause_start_classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.cause_end_classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.effect_start_classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.effect_end_classifier = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Causal relation classification\n",
    "        self.relation_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 3)  # No relation, Cause->Effect, Effect->Cause\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    \n",
    "    def create_padding_mask(self, x, pad_idx=0):\n",
    "        # x: [batch_size, seq_len]\n",
    "        # Returns a mask where pad tokens are 0, others are 1\n",
    "        return (x != pad_idx).float()\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        # text: [batch_size, seq_len]\n",
    "        batch_size = text.shape[0]\n",
    "        seq_len = text.shape[1]\n",
    "        \n",
    "        # Create padding mask\n",
    "        padding_mask = self.create_padding_mask(text).unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Get embeddings\n",
    "        embedded = self.embedding(text)                              # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # Pack padded sequence for LSTM\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "        \n",
    "        # Run through LSTM\n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Unpack the sequence\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True, total_length=seq_len)\n",
    "        # outputs: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Apply context layers\n",
    "        x = outputs\n",
    "        for i in range(len(self.context_layers)):\n",
    "            # Apply context layer\n",
    "            context_input = self.context_norms[i](x)\n",
    "            context_output = self.context_layers[i](context_input, context_input, context_input)\n",
    "            x = x + context_output\n",
    "            \n",
    "            # Apply feed-forward layer\n",
    "            ff_input = self.ff_norms[i](x)\n",
    "            ff_output = self.ff_layers[i](ff_input)\n",
    "            x = x + ff_output\n",
    "        \n",
    "        # Get mask predictions\n",
    "        cause_start_logits = self.cause_start_classifier(x).squeeze(-1)  # [batch_size, seq_len]\n",
    "        cause_end_logits = self.cause_end_classifier(x).squeeze(-1)      # [batch_size, seq_len]\n",
    "        effect_start_logits = self.effect_start_classifier(x).squeeze(-1)  # [batch_size, seq_len]\n",
    "        effect_end_logits = self.effect_end_classifier(x).squeeze(-1)      # [batch_size, seq_len]\n",
    "        \n",
    "        # Apply padding mask\n",
    "        padding_mask = padding_mask.squeeze(-1)  # [batch_size, seq_len]\n",
    "        cause_start_logits = cause_start_logits * padding_mask - 1e10 * (1 - padding_mask)\n",
    "        cause_end_logits = cause_end_logits * padding_mask - 1e10 * (1 - padding_mask)\n",
    "        effect_start_logits = effect_start_logits * padding_mask - 1e10 * (1 - padding_mask)\n",
    "        effect_end_logits = effect_end_logits * padding_mask - 1e10 * (1 - padding_mask)\n",
    "        \n",
    "        # For relation classification, we use the representation of the [CLS] token\n",
    "        # (which is the first token in the sequence)\n",
    "        sentence_rep = torch.mean(x, dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Get cause and effect representations\n",
    "        # We'll use the mean of token representations in the cause/effect spans\n",
    "        cause_probs = torch.softmax(cause_start_logits, dim=-1).unsqueeze(-1)    # [batch_size, seq_len, 1]\n",
    "        effect_probs = torch.softmax(effect_start_logits, dim=-1).unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Weighted sum of token representations\n",
    "        cause_rep = torch.sum(x * cause_probs, dim=1)    # [batch_size, hidden_dim]\n",
    "        effect_rep = torch.sum(x * effect_probs, dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Concatenate for relation classification\n",
    "        relation_input = torch.cat([cause_rep, effect_rep], dim=-1)  # [batch_size, hidden_dim*2]\n",
    "        relation_logits = self.relation_classifier(relation_input)   # [batch_size, 3]\n",
    "        \n",
    "        return {\n",
    "            \"cause_start_logits\": cause_start_logits,\n",
    "            \"cause_end_logits\": cause_end_logits,\n",
    "            \"effect_start_logits\": effect_start_logits,\n",
    "            \"effect_end_logits\": effect_end_logits,\n",
    "            \"relation_logits\": relation_logits\n",
    "        }\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Training Functions\n",
    "# ====================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            texts = batch[\"text\"].to(device)\n",
    "            text_lengths = batch[\"text_lengths\"]\n",
    "            cause_masks = batch[\"cause_masks\"].float().to(device)\n",
    "            effect_masks = batch[\"effect_masks\"].float().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(texts, text_lengths)\n",
    "            \n",
    "            # Calculating loss\n",
    "            cause_start_loss = criterion(outputs[\"cause_start_logits\"], cause_masks)\n",
    "            cause_end_loss = criterion(outputs[\"cause_end_logits\"], cause_masks)\n",
    "            effect_start_loss = criterion(outputs[\"effect_start_logits\"], effect_masks)\n",
    "            effect_end_loss = criterion(outputs[\"effect_end_logits\"], effect_masks)\n",
    "            \n",
    "            # Total loss (we're not using relation loss here since we focus on span detection)\n",
    "            loss = cause_start_loss + cause_end_loss + effect_start_loss + effect_end_loss\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts = batch[\"text\"].to(device)\n",
    "                text_lengths = batch[\"text_lengths\"]\n",
    "                cause_masks = batch[\"cause_masks\"].float().to(device)\n",
    "                effect_masks = batch[\"effect_masks\"].float().to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(texts, text_lengths)\n",
    "                \n",
    "                # Calculate loss\n",
    "                cause_start_loss = criterion(outputs[\"cause_start_logits\"], cause_masks)\n",
    "                cause_end_loss = criterion(outputs[\"cause_end_logits\"], cause_masks)\n",
    "                effect_start_loss = criterion(outputs[\"effect_start_logits\"], effect_masks)\n",
    "                effect_end_loss = criterion(outputs[\"effect_end_logits\"], effect_masks)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = cause_start_loss + cause_end_loss + effect_start_loss + effect_end_loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def extract_spans(logits, text_tokens, threshold=0.5):\n",
    "    \"\"\"Extract spans from token-level logits.\"\"\"\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    spans = []\n",
    "    \n",
    "    current_span = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        if prob > threshold:\n",
    "            current_span.append(i)\n",
    "        elif current_span:\n",
    "            start = current_span[0]\n",
    "            end = current_span[-1]\n",
    "            spans.append((start, end))\n",
    "            current_span = []\n",
    "    \n",
    "    # Adding the last span if it exists\n",
    "    if current_span:\n",
    "        start = current_span[0]\n",
    "        end = current_span[-1]\n",
    "        spans.append((start, end))\n",
    "    \n",
    "    return spans\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    \n",
    "    cause_predictions = []\n",
    "    effect_predictions = []\n",
    "    true_causes = []\n",
    "    true_effects = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            texts = batch[\"text\"].to(device)\n",
    "            text_lengths = batch[\"text_lengths\"]\n",
    "            cause_masks = batch[\"cause_masks\"]\n",
    "            effect_masks = batch[\"effect_masks\"]\n",
    "            original_texts = batch[\"original_texts\"]\n",
    "            original_causes = batch[\"original_causes\"]\n",
    "            original_effects = batch[\"original_effects\"]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(texts, text_lengths)\n",
    "            \n",
    "            # Get predictions\n",
    "            for i in range(len(texts)):\n",
    "                # Get the text length for this sample\n",
    "                text_len = text_lengths[i].item()\n",
    "                \n",
    "                # Get logits for this sample\n",
    "                cause_start_logits = outputs[\"cause_start_logits\"][i][:text_len]\n",
    "                effect_start_logits = outputs[\"effect_start_logits\"][i][:text_len]\n",
    "                \n",
    "                # Extract spans\n",
    "                cause_spans = extract_spans(cause_start_logits, original_texts[i], threshold)\n",
    "                effect_spans = extract_spans(effect_start_logits, original_texts[i], threshold)\n",
    "                \n",
    "                # Add predictions and ground truth\n",
    "                cause_predictions.append(cause_spans)\n",
    "                effect_predictions.append(effect_spans)\n",
    "                true_causes.append((original_causes[i], cause_masks[i]))\n",
    "                true_effects.append((original_effects[i], effect_masks[i]))\n",
    "    \n",
    "    return {\n",
    "        \"cause_predictions\": cause_predictions,\n",
    "        \"effect_predictions\": effect_predictions,\n",
    "        \"true_causes\": true_causes,\n",
    "        \"true_effects\": true_effects\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_causal_relation(model, text, vocab, device):\n",
    "    \"\"\"Predict cause and effect in a given text.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and numericalize the text\n",
    "    numeric_text = vocab.numericalize(text)\n",
    "    text_tensor = torch.tensor(numeric_text).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "    text_length = torch.tensor([len(numeric_text)])\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(text_tensor, text_length)\n",
    "    \n",
    "    # Get predictions\n",
    "    cause_start_logits = outputs[\"cause_start_logits\"][0]\n",
    "    cause_end_logits = outputs[\"cause_end_logits\"][0]\n",
    "    effect_start_logits = outputs[\"effect_start_logits\"][0]\n",
    "    effect_end_logits = outputs[\"effect_end_logits\"][0]\n",
    "    relation_logits = outputs[\"relation_logits\"][0]\n",
    "    \n",
    "    # Get cause and effect spans\n",
    "    cause_start_probs = torch.sigmoid(cause_start_logits).cpu().numpy()\n",
    "    cause_end_probs = torch.sigmoid(cause_end_logits).cpu().numpy()\n",
    "    effect_start_probs = torch.sigmoid(effect_start_logits).cpu().numpy()\n",
    "    effect_end_probs = torch.sigmoid(effect_end_logits).cpu().numpy()\n",
    "    \n",
    "    # Extract spans\n",
    "    cause_spans = extract_spans(cause_start_logits, text)\n",
    "    effect_spans = extract_spans(effect_start_logits, text)\n",
    "    \n",
    "    # Get relation type\n",
    "    relation_type = torch.argmax(relation_logits).item()\n",
    "    relation_types = [\"No relation\", \"Cause->Effect\", \"Effect->Cause\"]\n",
    "    \n",
    "    # Converting token indices back to words\n",
    "    text_tokens = vocab.tokenize(text)\n",
    "    \n",
    "    causes = []\n",
    "    for start, end in cause_spans:\n",
    "        if start < len(text_tokens) and end < len(text_tokens):\n",
    "            cause_text = \" \".join(text_tokens[start:end+1])\n",
    "            causes.append(cause_text)\n",
    "    \n",
    "    effects = []\n",
    "    for start, end in effect_spans:\n",
    "        if start < len(text_tokens) and end < len(text_tokens):\n",
    "            effect_text = \" \".join(text_tokens[start:end+1])\n",
    "            effects.append(effect_text)\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"causes\": causes,\n",
    "        \"effects\": effects,\n",
    "        \"relation_type\": relation_types[relation_type],\n",
    "        \"cause_confidence\": cause_start_probs.max() if len(cause_start_probs) > 0 else 0,\n",
    "        \"effect_confidence\": effect_start_probs.max() if len(effect_start_probs) > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Main Function\n",
    "# ====================================\n",
    "\n",
    "def main(csv_path, save_dir=\"financial_model\", test_sentences=None):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Preparing data\n",
    "    vocab_path = os.path.join(save_dir, \"financial_vocab.pkl\")\n",
    "    train_dataset, test_dataset, vocab = prepare_data(csv_path, test_size=0.2, vocab_save_path=vocab_path)\n",
    "    \n",
    "    # Creating data loaders\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_batch\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_batch\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initializing model\n",
    "    model = FinancialCausalDetector(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=300,\n",
    "        hidden_dim=768,\n",
    "        num_layers=3,\n",
    "        num_heads=8,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setting up optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training the model\n",
    "    model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        num_epochs=20\n",
    "    )\n",
    "    \n",
    "    # Saving the trained model\n",
    "    model_path = os.path.join(save_dir, \"financial_causal_model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': len(vocab),\n",
    "        'embedding_dim': 300,\n",
    "        'hidden_dim': 768,\n",
    "        'num_layers': 3,\n",
    "        'num_heads': 8,\n",
    "        'dropout': 0.3\n",
    "    }, model_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Evaluating model\n",
    "    eval_results = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    if test_sentences:\n",
    "        print(\"\\nMaking predictions on test sentences:\")\n",
    "        for sentence in test_sentences:\n",
    "            prediction = predict_causal_relation(model, sentence, vocab, device)\n",
    "            print(f\"\\nText: {prediction['text']}\")\n",
    "            print(f\"Detected causes: {prediction['causes']}\")\n",
    "            print(f\"Detected effects: {prediction['effects']}\")\n",
    "            print(f\"Relation type: {prediction['relation_type']}\")\n",
    "            print(f\"Cause confidence: {prediction['cause_confidence']:.4f}\")\n",
    "            print(f\"Effect confidence: {prediction['effect_confidence']:.4f}\")\n",
    "    \n",
    "    return model, vocab\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Model Loading and Inference\n",
    "# ====================================\n",
    "\n",
    "def load_model(model_path, vocab_path, device=None):\n",
    "    \"\"\"Load a trained model and vocabulary.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Loading vocabulary\n",
    "    vocab = Vocabulary.load_vocab(vocab_path)\n",
    "    \n",
    "    # Loading model configuration and state\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Initializing model with the same configuration\n",
    "    model = FinancialCausalDetector(\n",
    "        vocab_size=checkpoint['vocab_size'],\n",
    "        embedding_dim=checkpoint['embedding_dim'],\n",
    "        hidden_dim=checkpoint['hidden_dim'],\n",
    "        num_layers=checkpoint['num_layers'],\n",
    "        num_heads=checkpoint['num_heads'],\n",
    "        dropout=checkpoint['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loading state dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, vocab\n",
    "\n",
    "\n",
    "def batch_predict(model, texts, vocab, device, batch_size=16):\n",
    "    \"\"\"Make predictions on a batch of texts.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_numeric = []\n",
    "        batch_lengths = []\n",
    "        \n",
    "        # Numericalize texts\n",
    "        for text in batch_texts:\n",
    "            numeric_text = vocab.numericalize(text)\n",
    "            batch_numeric.append(torch.tensor(numeric_text))\n",
    "            batch_lengths.append(len(numeric_text))\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_texts = pad_sequence(batch_numeric, batch_first=True, padding_value=0)\n",
    "        text_lengths = torch.tensor(batch_lengths)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(padded_texts.to(device), text_lengths)\n",
    "        \n",
    "        # Process each prediction\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            text_len = text_lengths[j].item()\n",
    "            \n",
    "            # Get logits for this sample\n",
    "            cause_start_logits = outputs[\"cause_start_logits\"][j][:text_len]\n",
    "            effect_start_logits = outputs[\"effect_start_logits\"][j][:text_len]\n",
    "            relation_logits = outputs[\"relation_logits\"][j]\n",
    "            \n",
    "            # Get cause and effect spans\n",
    "            cause_spans = extract_spans(cause_start_logits, text)\n",
    "            effect_spans = extract_spans(effect_start_logits, text)\n",
    "            \n",
    "            # Get relation type\n",
    "            relation_type = torch.argmax(relation_logits).item()\n",
    "            relation_types = [\"No relation\", \"Cause->Effect\", \"Effect->Cause\"]\n",
    "            \n",
    "            # Convert token indices back to words\n",
    "            text_tokens = vocab.tokenize(text)\n",
    "            \n",
    "            causes = []\n",
    "            for start, end in cause_spans:\n",
    "                if start < len(text_tokens) and end < len(text_tokens):\n",
    "                    cause_text = \" \".join(text_tokens[start:end+1])\n",
    "                    causes.append(cause_text)\n",
    "            \n",
    "            effects = []\n",
    "            for start, end in effect_spans:\n",
    "                if start < len(text_tokens) and end < len(text_tokens):\n",
    "                    effect_text = \" \".join(text_tokens[start:end+1])\n",
    "                    effects.append(effect_text)\n",
    "            \n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"causes\": causes,\n",
    "                \"effects\": effects,\n",
    "                \"relation_type\": relation_types[relation_type]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Context Processing\n",
    "# ====================================\n",
    "\n",
    "class FinancialContextProcessor:\n",
    "    \"\"\"Process financial text to enhance causal detection with domain knowledge.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, device):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        \n",
    "        # Financial domain-specific patterns\n",
    "        self.financial_keywords = [\n",
    "            'revenue', 'profit', 'loss', 'growth', 'decline', 'increase', 'decrease',\n",
    "            'market', 'stock', 'share', 'price', 'investment', 'investor', 'dividend',\n",
    "            'earning', 'quarter', 'fiscal', 'report', 'forecast', 'outlook', 'guidance',\n",
    "            'volatility', 'inflation', 'interest rate', 'debt', 'asset', 'liability',\n",
    "            'acquisition', 'merger', 'bankruptcy', 'default', 'credit', 'loan'\n",
    "        ]\n",
    "        \n",
    "        self.causal_connectors = [\n",
    "            'because', 'due to', 'as a result of', 'therefore', 'consequently',\n",
    "            'hence', 'thus', 'leads to', 'causes', 'results in', 'affects',\n",
    "            'influences', 'impacts', 'drives', 'triggered by', 'stemming from'\n",
    "        ]\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Apply domain-specific preprocessing.\"\"\"\n",
    "        # Normalize text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Special handling for financial abbreviations and terms\n",
    "        # Replace common abbreviations with full forms\n",
    "        text = text.replace('q1', 'quarter one')\n",
    "        text = text.replace('q2', 'quarter two')\n",
    "        text = text.replace('q3', 'quarter three')\n",
    "        text = text.replace('q4', 'quarter four')\n",
    "        text = text.replace('fy', 'fiscal year')\n",
    "        text = text.replace('yoy', 'year over year')\n",
    "        text = text.replace('mom', 'month over month')\n",
    "        text = text.replace('qoq', 'quarter over quarter')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def enhance_context(self, text):\n",
    "        \"\"\"Enhance the text with financial context markers.\"\"\"\n",
    "        # Find financial terms\n",
    "        enhanced_text = text\n",
    "        for keyword in self.financial_keywords:\n",
    "            if keyword in text.lower():\n",
    "                # Highlight financial terms (in a real implementation, \n",
    "                # this could involve more sophisticated techniques)\n",
    "                enhanced_text = enhanced_text.replace(keyword, f\"<FIN>{keyword}</FIN>\")\n",
    "        \n",
    "        # Highlight causal connectors\n",
    "        for connector in self.causal_connectors:\n",
    "            if connector in text.lower():\n",
    "                enhanced_text = enhanced_text.replace(connector, f\"<CAUSE>{connector}</CAUSE>\")\n",
    "        \n",
    "        return enhanced_text\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Analyze text for financial causal relations with enhanced context.\"\"\"\n",
    "        # Preprocess\n",
    "        preprocessed_text = self.preprocess_text(text)\n",
    "        \n",
    "        # Enhance context (this step would integrate with the model in a real implementation)\n",
    "        enhanced_text = self.enhance_context(preprocessed_text)\n",
    "        \n",
    "        # Standard prediction\n",
    "        prediction = predict_causal_relation(self.model, preprocessed_text, self.vocab, self.device)\n",
    "        \n",
    "        # Add domain-specific analysis\n",
    "        financial_terms = []\n",
    "        for keyword in self.financial_keywords:\n",
    "            if keyword in text.lower():\n",
    "                financial_terms.append(keyword)\n",
    "        \n",
    "        causal_markers = []\n",
    "        for connector in self.causal_connectors:\n",
    "            if connector in text.lower():\n",
    "                causal_markers.append(connector)\n",
    "        \n",
    "        prediction.update({\n",
    "            \"financial_terms\": financial_terms,\n",
    "            \"causal_markers\": causal_markers,\n",
    "            \"enhanced_text\": enhanced_text\n",
    "        })\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Usage\n",
    "# ====================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"updated_final_dataset.csv\"  \n",
    "    save_dir = \"financial_model\"\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"The company's stock price fell sharply because of the disappointing earnings report.\",\n",
    "        \"Due to the increase in interest rates, mortgage applications decreased by 10% last month.\",\n",
    "        \"Revenue growth in the technology sector accelerated as a result of increased consumer spending on electronics.\",\n",
    "        \"The market volatility was triggered by concerns about inflation.\",\n",
    "        \"The corporate tax cut led to higher profit margins for many companies.\",\n",
    "        \"Due to the increase in tax rates, the company's profit margin decreased by 5 percent.\",\n",
    "        \"The stock price dropped 10 percent when the CEO resigned unexpectedly.\",\n",
    "        \"As interest rates fell, mortgage applications increased by 20 percent.\",\n",
    "        \"The merger with ABC Corp resulted in a 15 percent increase in market share.\",\n",
    "        \"The company's revenue grew by 8 percent because they expanded into international markets.\",\n",
    "        \"Because Oshrad is a privately held business, it was liable for a 26.3% company tax on the sale.\",\n",
    "        \"The Israel Tax Authority claims that when a company controlled by the Brosh brothers sold a 14% stake in Oshrad Natural Gas for 8.6 million shekels in 2017, they should have paid 4.3 million shekels in tax.\",\n",
    "        \n",
    "        \"The collapse of Silicon Bank led to widespread investor panic and a temporary dip in the tech sector.\",\n",
    "        \"Following the announcement of record-high inflation, consumer confidence fell to its lowest level in a decade.\",\n",
    "        \"Strong quarterly earnings prompted a surge in the company's stock price.\",\n",
    "        \"The central bank’s decision to cut interest rates boosted lending activity across the housing market.\",\n",
    "        \"The introduction of new tariffs on Chinese goods caused import costs to rise significantly for U.S. retailers.\",\n",
    "        \"A downgrade in the country’s credit rating triggered a sell-off in government bonds.\",\n",
    "        \"Due to a cybersecurity breach, the fintech company lost $10 million in market value overnight.\",\n",
    "        \"The announcement of a new product line caused investor optimism and lifted share prices by 12%.\",\n",
    "        \"Because of reduced oil supply from OPEC countries, global fuel prices surged to a five-year high.\",\n",
    "        \"Rising raw material costs led manufacturers to increase prices across multiple sectors.\"\n",
    "    ]\n",
    "\n",
    "    # Training and save model\n",
    "    model, vocab = main(csv_path, save_dir, test_sentences)\n",
    "    \n",
    "    # Create financial insights analyzer\n",
    "    insights_analyzer = FinancialCausalInsights(model, vocab, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    # Analyze a financial text\n",
    "    financial_text = \"The Federal Reserve's decision to raise interest rates caused significant declines in growth stocks, leading many investors to shift their portfolios to value stocks.\"\n",
    "    insights = insights_analyzer.analyze_financial_impact(financial_text)\n",
    "    \n",
    "    print(\"\\nFinancial Insights:\")\n",
    "    print(f\"Text: {insights['text']}\")\n",
    "    print(f\"Overall Sentiment: {insights['overall_sentiment']}\")\n",
    "    print(\"\\nCausal Chains:\")\n",
    "    for chain in insights['causal_chains']:\n",
    "        print(f\"  {chain['cause']} ({chain['cause_category']}, {chain['cause_impact']}) → {chain['effect']} ({chain['effect_category']}, {chain['effect_impact']})\")\n",
    "    \n",
    "    print(\"\\nEntities:\")\n",
    "    for entity in insights['entities']:\n",
    "        print(f\"  {entity['name']} - {entity['category']} - {entity['impact']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b70c5-03d8-471d-ac8d-a5118c8637ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
